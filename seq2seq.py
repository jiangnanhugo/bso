import theano
import numpy as np
import theano.tensor as T
from layers.recurrent import *
from softmax import *

if theano.config.device == 'cpu':
    from theano.tensor.shared_randomstreams import RandomStreams
else:
    from theano.sandbox.cuda.rng_curand import CURAND_RandomStreams as RandomStreams

from updates import *
from utils import norm_weight

import logging

logger = logging.getLogger()
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.setLevel(logging.DEBUG)


class Seq2Seq(object):
    def __init__(self, envocab_size, devocab_size, n_hidden, rnn_cells=None,
                  optimizer='adam',dropout=0.1):
        self.enc = T.imatrix('batch sequence encoder input')
        self.enc_mask = T.fmatrix('batch sequence enc mask')
        self.dec_input = T.imatrix('batch sequence decoder input')
        self.dec_imask = T.fmatrix('batch sequence decoder mask')
        self.dec_output = T.imatrix('batch sequence decoder input mask')
        self.dec_omask = T.fmatrix('batch sequence decoder output mask')

        self.envocab_size = envocab_size
        self.n_hidden = n_hidden
        self.devocab_size = devocab_size

        self.en_cell,self.de_cell=rnn_cells

        self.optimizer = optimizer
        self.dropout = dropout

        self.en_loopup_table = theano.shared(name='Encoder look-up table',
                                             value=norm_weight(envocab_size, n_hidden),
                                             borrow=True)
        self.de_loopup_table = theano.shared(name='Decoder look-up table',
                                             value=norm_weight(devocab_size, n_hidden),
                                             borrow=True)
        self.is_train = T.iscalar('is_train')

        self.rng = RandomStreams(1234)
        self.build_graph()

    def build_graph(self):
        logger.debug('build rnn cell')
        embd_enc = self.en_loopup_table[self.enc]
        embd_dec_input = self.de_loopup_table[self.dec_input]

        # encoder: bidrection RNN
        # word embedding for forward rnn (source)
        encoder = None
        if self.en_cell == 'bilstm':
            encoder = BiLSTM(self.rng, self.envocab_size, self.n_hidden,
                             embd_enc, self.enc_mask,
                             output_mode='sum', is_train=self.is_train, dropout=self.dropout)
        elif self.en_cell == 'bigru':
            encoder = BiGRU(self.rng, self.envocab_size, self.n_hidden,
                            embd_enc, self.enc_mask,
                            output_mode='sum', is_train=self.is_train, dropout=self.dropout)
        elif self.en_cell.startswith('rnnblock'):
            mode = self.en_cell.split('.')[-1]
            print mode
            encoder = RnnBlock(self.rng, self.n_hidden,
                               embd_enc, self.enc_mask,
                               self.is_train, self.dropout, mode=mode)

        ctx = encoder.context
        # decoder init state
        init_state = encoder.activation
        decoder = None
        if self.de_cell == 'attlstm':
            decoder = CondLSTM(self.rng,
                               self.devocab_size, self.n_hidden,
                               embd_dec_input, self.dec_imask,
                               init_state=init_state, context=ctx, context_mask=self.enc_mask,
                               is_train=self.is_train, dropout=self.dropout)
        elif self.de_cell == 'attgru':
            decoder = AttGRU(self.rng,
                             self.devocab_size, self.n_hidden,
                             embd_dec_input, self.dec_imask,
                             init_state=init_state, context=ctx, context_mask=self.enc_mask,
                             is_train=self.is_train, dropout=self.dropout)

        # hidden states of the decoder gru
        hidden_states=decoder.activation[0]
        # weighted averages of context, generated by attention module
        contexts=decoder.activation[1]
        # weights (alignment matrix)
        alignment_matries=decoder.activation[2]

        output_layer = comb_softmax(self.n_hidden, self.devocab_size, hidden_states,contexts)
        cost = self.categorical_crossentropy(output_layer.activation, self.dec_output, self.dec_omask)
        self.params = [encoder.params, decoder.params]

        lr = T.scalar('lr')
        gparams = [T.clip(T.grad(cost, p) - 3, 3) for p in self.params]
        updates = None
        if self.optimizer == 'sgd':
            updates = sgd(self.params, gparams, lr)
        elif self.optimizer == 'adam':
            updates = adam(self.params, gparams, lr)
        elif self.optimizer == 'rmsprop':
            updates = rmsprop(self.params, gparams, lr)

        self.train = theano.function(inputs=[self.enc, self.enc_mask,
                                             self.dec_input, self.dec_imask,self.dec_output,self.dec_omask, lr],
                                     outputs=cost,
                                     updates=updates,
                                     givens={self.is_train: np.cast['int32'](1)})

    def categorical_crossentropy(self, y_pred, y_true, y_mask):
        y_true = y_true.flatten()
        mask = y_mask.flatten()
        nll = T.nnet.categorical_crossentropy(y_pred, y_true)
        batch_nll = T.sum(nll * mask)
        return batch_nll / T.sum(mask)
