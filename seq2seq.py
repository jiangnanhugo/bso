import theano
import numpy as np
import theano.tensor as T
from layers.recurrent import *
from layers.softmax import *

if theano.config.device == 'cpu':
    from theano.tensor.shared_randomstreams import RandomStreams
else:
    # from theano.sandbox.cuda.rng_curand import CURAND_RandomStreams as RandomStreams
    from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams

from updates import *
from layers.utils import norm_weight

import logging

logger = logging.getLogger()
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.setLevel(logging.DEBUG)


class Seq2Seq(object):
    def __init__(self, envocab_size, devocab_size, n_hidden, rnn_cells=None,
                 optimizer='adam', dropout=0.1):
        self.enc = T.imatrix('encoder input')
        self.enc_mask = T.fmatrix('enc mask')
        self.dec_input = T.imatrix('decoder input')
        self.dec_output = T.imatrix('decoder output')
        self.dec_mask = T.fmatrix('decoder mask')

        self.envocab_size = envocab_size
        self.n_hidden = n_hidden
        self.devocab_size = devocab_size

        self.en_cell, self.de_cell = rnn_cells

        self.optimizer = optimizer
        self.dropout = dropout

        self.en_loopup_table = norm_weight(envocab_size, n_hidden, name='Encoder look-up table')
        self.de_loopup_table = norm_weight(devocab_size, n_hidden, name='Decoder look-up table')

        self.is_train = T.iscalar('is_train')
        self.rng = RandomStreams(1234)
        self.build_graph()

    def build_graph(self):
        logger.debug('build rnn cell....')
        enc_shape = self.enc.shape
        embd_enc = self.en_loopup_table[self.enc.flatten()]
        embd_enc = embd_enc.reshape([enc_shape[0], enc_shape[1], -1])
        emc_input = self.en_loopup_table[self.enc, :]
        dec_shape = self.dec_input.shape
        embd_dec_input = self.de_loopup_table[self.dec_input.flatten()]
        embd_dec_input = embd_dec_input.reshape([dec_shape[0], dec_shape[1], -1])

        # encoder: bidrection RNN
        # word embedding for forward rnn (source)
        logger.debug('calculating bidirectional encoder.....')
        encoder = None
        if self.en_cell == 'bilstm':
            encoder = BiLSTM(self.rng, self.n_hidden, self.n_hidden,
                             embd_enc, self.enc_mask,
                             output_mode='sum', is_train=self.is_train, dropout=self.dropout)
        elif self.en_cell == 'bigru':
            encoder = BiGRU(self.rng, self.n_hidden, self.n_hidden,
                            embd_enc, self.enc_mask,
                            is_train=self.is_train, dropout=self.dropout)
        elif self.en_cell == 'gru':
            encoder = GRU(self.rng, self.n_hidden, self.n_hidden,
                          embd_enc, self.enc_mask,
                          is_train=self.is_train, dropout=self.dropout)
        elif self.en_cell.startswith('rnnblock'):
            mode = self.en_cell.split('.')[-1]
            print mode
            encoder = RnnBlock(self.rng, self.n_hidden,
                               embd_enc, self.enc_mask,
                               self.is_train, self.dropout, mode=mode)

        ctx = encoder.context
        # decoder init state
        init_state = encoder.activation
        logger.debug('calculating decoder with attention model')
        decoder = None
        if self.de_cell == 'attlstm':
            decoder = CondLSTM(self.rng,
                               self.n_hidden, self.n_hidden,
                               embd_dec_input, self.dec_mask,
                               init_state=init_state, context=ctx, context_mask=self.enc_mask,
                               is_train=self.is_train, dropout=self.dropout)
        elif self.de_cell == 'attgru':
            decoder = AttGRU(self.rng,
                             self.n_hidden, self.n_hidden,
                             embd_dec_input, self.dec_mask,
                             init_state=init_state, context=ctx, context_mask=self.enc_mask,
                             is_train=self.is_train, dropout=self.dropout, dimctx=self.n_hidden)

        # hidden states of the decoder gru
        hidden_states = decoder.hidden_states
        # weighted averages of context, generated by attention module
        contexts = decoder.contexts
        # weights (alignment matrix)
        # alignment_matries = decoder.activation[2]

        # output_layer = comb_softmax(self.n_hidden, self.devocab_size, hidden_states, contexts,dimctx=self.n_hidden*2)
        output_layer = softmax(self.n_hidden, self.devocab_size, hidden_states)

        cost, acc = self.categorical_crossentropy(output_layer, self.dec_output, self.dec_mask)
        self.params = [self.en_loopup_table, self.de_loopup_table]
        self.params += encoder.params
        self.params += decoder.params

        self.debug = theano.function(inputs=[self.enc, self.enc_mask,self.dec_input, self.dec_mask],
                                     outputs=[ctx, init_state,decoder.hidden_states])

        logger.debug('calculating gradient update')

        lr = T.scalar('lr')
        gparams = [T.clip(T.grad(cost, p), - 3., 3.) for p in self.params]
        updates = None
        if self.optimizer == 'sgd':
            updates = sgd(self.params, gparams, lr)
        elif self.optimizer == 'adam':
            updates = adam(self.params, gparams, lr)
        elif self.optimizer == 'rmsprop':
            updates = rmsprop(self.params, gparams, lr)

        logger.debug('compling final function......')
        self.train = theano.function(inputs=[self.enc, self.enc_mask,
                                             self.dec_input, self.dec_output, self.dec_mask, lr],
                                     outputs=[cost, acc],
                                     updates=updates)
                                     #givens={self.is_train: np.cast['int32'](1)})

    def categorical_crossentropy(self, output, y_true, y_mask):
        y_prob = output.activation.flatten()
        y_pred = output.predict.flatten()
        mask = y_mask.flatten()
        y_true = y_true.flatten()
        nll = T.nnet.categorical_crossentropy(y_prob, y_true)
        batch_nll = T.sum(nll * mask)
        batch_acc = T.sum(T.eq(y_pred, y_true) * mask)
        return batch_nll / T.sum(mask), batch_acc / T.sum(mask)
